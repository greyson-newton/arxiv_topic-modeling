{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13a012-db11-496d-a808-86d779b709ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"spacy\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import dask.bag as db\n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from IPython.utils import io\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS #import commen list of stopword\n",
    "import en_core_sci_lg  # import downlaoded model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import metrics\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7df9b80-767e-45ee-8318-a9b67835087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.read_text('./arxiv_data.json').map(json.loads)\n",
    "\n",
    "get_latest_version = lambda x: x['versions'][-1]['created']\n",
    "\n",
    "\n",
    "# get only necessary fields of the metadata file\n",
    "trim = lambda x: {'id': x['id'],\n",
    "                  'authors': x['authors'],\n",
    "                  'title': x['title'],\n",
    "                  'doi': x['doi'],\n",
    "                  'category':x['categories'].split(' '),\n",
    "                  'abstract':x['abstract'],}\n",
    "# filter for papers published on or after 2019-01-01\n",
    "columns = ['id','category','abstract']\n",
    "docs_df = (docs.filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2018)\n",
    "           .map(trim).\n",
    "           compute())\n",
    "\n",
    "# convert to pandas\n",
    "docs_df = pd.DataFrame(docs_df)\n",
    "\n",
    "docs_df.to_csv(\"trimmed_arxiv_docs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b138aef5-3177-40a2-9049-457319680594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\greyn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\greyn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\util.py:730: UserWarning: [W095] Model 'en_core_sci_lg' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [15:22<00:00, 10.84it/s]\n"
     ]
    }
   ],
   "source": [
    "df=docs_df\n",
    "df['abstract_word_count'] = docs_df['abstract'].apply(lambda x: len(x.strip().split())) \n",
    "\n",
    "df=pd.read_csv('trimmed_arxiv_docs.csv')\n",
    "df.drop_duplicates(['abstract',], inplace=True)\n",
    "\n",
    "df = df.sample(10000, random_state=42)\n",
    "\n",
    "# Parser\n",
    "parser = en_core_sci_lg.load()\n",
    "parser.max_length = 7000000 #Limit the size of the parser\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    ''' Function to preprocess text of scientific papers \n",
    "        (e.g Removing Stopword and puntuations)'''\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] # transform to lowercase and then split the scentence\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ] #remove stopsword an punctuation\n",
    "    mytokens = \" \".join([i for i in mytokens]) \n",
    "    return mytokens\n",
    "\n",
    "punctuations = string.punctuation #list of punctuation to remove from text\n",
    "stopwords = list(STOP_WORDS)\n",
    "stopwords[:10]   \n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "df[\"processed_text\"] = df[\"abstract\"].progress_apply(spacy_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581a0c4e-c77d-422c-a62b-70668bafc5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2798)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(text, maxx_features):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=maxx_features)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X \n",
    "\n",
    "text = df['processed_text'].values\n",
    "X = vectorize(text, 2 ** 12) #arbitrary max feature -_> Hyperpara. for optimisation (?)    \n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42) #Keep 95% of the variance\n",
    "X_reduced= pca.fit_transform(X.toarray())\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26640f94-377e-4ac6-99b1-7b95644fdd6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18420/1906462479.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     pipe_pca_kmean = Pipeline([(\"cluster\", KMeans(n_clusters=n_clusters, random_state=r_seed, verbose=0, n_jobs=1))]\n\u001b[0m\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# find optimal k value\n",
    "r_seed = 24\n",
    "cluster_errors = []\n",
    "\n",
    "for i in range(1, 50):\n",
    "    n_clusters = i\n",
    "    i+=5\n",
    "    pipe_pca_kmean = Pipeline([(\"cluster\", KMeans(n_clusters=n_clusters, random_state=r_seed, verbose=0, n_jobs=1))]\n",
    "    )\n",
    "\n",
    "    pipe_pca_kmean.fit(X_reduced)\n",
    "    pipe_pca_kmean.predict(X_reduced)\n",
    "    cluster_errors.append(pipe_pca_kmean.named_steps[\"cluster\"].inertia_) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cec61d-3fb7-42b3-93ac-c2aab5a024f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.clf()\n",
    "plt.plot(cluster_errors, \"o-\")\n",
    "plt.xlabel(\"k_clusters\")\n",
    "plt.ylabel(\"sum sq distances from mean\")\n",
    "plt.savefig('./cluster_kmeans_error.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_conda",
   "language": "python",
   "name": "ex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
